{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93ff32a1",
   "metadata": {},
   "source": [
    "# Code_SMS Spam Classifier using Decision Trees and Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3891a57f",
   "metadata": {},
   "source": [
    "Let us now use the various NLP techniques that we have learnt to build a SMS Spam classifier.\n",
    "\n",
    "In order to build the classifier, we have collected historic data which has different SMS messages marked as 'ham' (not-spam) or 'spam'. You can download this data from here .\n",
    "\n",
    "To build the model, we shall follow the below steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6381ee2c",
   "metadata": {},
   "source": [
    "# Step 1: Load the data into the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "928b5491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sms_data: \n",
      "      v1                                                 v2 Unnamed: 2  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
      "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
      "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
      "\n",
      "  Unnamed: 3 Unnamed: 4  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n",
      "(5572, 2)\n",
      "  Value                                               Text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: Value, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Loading the data into the environment using pandas\n",
    "# Note: Please use appropriate filename and path\n",
    "sms_data = pd.read_csv(\"spam.csv\", encoding='latin-1')\n",
    "# Review the loaded data\n",
    "print('sms_data: \\n',sms_data.head())\n",
    "cols = sms_data.columns[:2]\n",
    "data = sms_data[cols]\n",
    "print(data.shape)\n",
    "data = data.rename(columns={\"v1\":\"Value\",\"v2\":\"Text\"})\n",
    "print(data.head())\n",
    "print(data.Value.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ac1426",
   "metadata": {},
   "source": [
    "# Step 2: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e7cf0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-75601db57bdc>:8: FutureWarning: Possible set intersection at position 5\n",
      "  data[\"Punctuations\"] = data[\"Text\"].apply(lambda x: len(re.findall(r\"[^\\w+&&^\\s]\",x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Value                                               Text  Punctuations  \\\n",
      "14   ham                I HAVE A DATE ON SUNDAY WITH WILL!!             2   \n",
      "15  spam  XXXMobileMovieClub: To use your credit, click ...            11   \n",
      "16   ham                         Oh k...i'm watching here:)             6   \n",
      "17   ham  Eh u remember how 2 spell his name... Yes i di...             5   \n",
      "18   ham  Fine if thatåÕs the way u feel. ThatåÕs the wa...             1   \n",
      "19  spam  England v Macedonia - dont miss the goals/team...             7   \n",
      "20   ham          Is that seriously how you spell his name?             1   \n",
      "21   ham  IÛ÷m going to try for 2 months ha ha only joking             2   \n",
      "22   ham  So Ì_ pay first lar... Then when is da stock c...             6   \n",
      "23   ham  Aft i finish my lunch then i go str down lor. ...             3   \n",
      "24   ham  Ffffffffff. Alright no way I can meet up with ...             2   \n",
      "\n",
      "    Phonenumbers  Links  Uppercase  unusualwords  \n",
      "14             0      0          8             0  \n",
      "15             0      1          1             3  \n",
      "16             0      0          0             0  \n",
      "17             0      0          0             0  \n",
      "18             0      0          0             2  \n",
      "19             0      0          2             6  \n",
      "20             0      0          0             0  \n",
      "21             0      0          0             2  \n",
      "22             0      0          1             1  \n",
      "23             0      0          1             4  \n",
      "24             0      0          1             1  \n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "punctuation = list(punctuation)\n",
    "# Creating a new feature called Punctuations. \n",
    "# This feature counts the number of punctuation characters in the sms message \n",
    "data[\"Punctuations\"] = data[\"Text\"].apply(lambda x: len(re.findall(r\"[^\\w+&&^\\s]\",x)))\n",
    "# Creating a new feature called Phonenumbers. \n",
    "# This feature indicates if the sms text contains a phonenumber or not\n",
    "data[\"Phonenumbers\"] = data[\"Text\"].apply(lambda x: len(re.findall(r\"[0-9]{10}\",x)))\n",
    "# Creating a new feature called Links.\n",
    "# This feature indicates if the sms text contains a URL or not \n",
    "is_link = lambda x: 1 if re.search(r\"https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\",x)!=None else 0\n",
    "data[\"Links\"] = data[\"Text\"].apply(is_link)\n",
    "# Creating a new feature called Uppercase.\n",
    "# This feature indicates how many words in the the sms text are in upper case\n",
    "count_upper = lambda x : list(map(str.isupper,x.split())).count(True) \n",
    "upper_case = lambda y,n : n+1 if y.isupper() else n\n",
    "data[\"Uppercase\"] = data[\"Text\"].apply(count_upper)\n",
    "# Identifying and counting how many unusual words are there in the sms text\n",
    "def find_unusual_words(text):\n",
    "    text_vocab_set = set(w.lower() for w in text if w.isalpha())\n",
    "    english_vocab_set = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    unusual_set = text_vocab_set - english_vocab_set\n",
    "    return len(sorted(unusual_set))\n",
    "data[\"unusualwords\"] = data[\"Text\"].apply(lambda x: find_unusual_words(word_tokenize(x)))\n",
    "# View a few records of the data after creating these features\n",
    "print(data[14:25])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45775ea6",
   "metadata": {},
   "source": [
    "In the above code snippet, we have created new features by understanding the content of our data. This is a critical exercise to undergo when you are building NLP applications.\n",
    "\n",
    "The below code snippet converts the text into a TF-IDF matrix. Recall that the TF-IDF matrix is a numeric representation of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67aadb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7558227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_idf= TfidfVectorizer(stop_words=\"english\",strip_accents='ascii',max_features=300)\n",
    "tf_idf_matrix = tf_idf.fit_transform(data[\"Text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8853a53",
   "metadata": {},
   "source": [
    "TF-IDF vectorization also does some of the required cleaning and normalization steps such as removing punctuation, removing stop words, removing accents, etc. In the above snippet, we have set the value of max_features to 300, indicating that the TF-IDF matrix contain only the 300 most common words in the text. Doing this reduces the dimensionality of the TF-IDF vector. \n",
    "\n",
    "Finally, the below code snippet, combines the TF-IDF matrix and the other features we created earlier into a single data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9318d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_extra_features = pd.concat([data,pd.DataFrame(tf_idf_matrix.toarray(),columns=tf_idf.get_feature_names())],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78f4bf49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "      <th>Text</th>\n",
       "      <th>Punctuations</th>\n",
       "      <th>Phonenumbers</th>\n",
       "      <th>Links</th>\n",
       "      <th>Uppercase</th>\n",
       "      <th>unusualwords</th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>150p</th>\n",
       "      <th>...</th>\n",
       "      <th>world</th>\n",
       "      <th>www</th>\n",
       "      <th>xmas</th>\n",
       "      <th>xxx</th>\n",
       "      <th>ya</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yo</th>\n",
       "      <th>yup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594379</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 307 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Value                                               Text  Punctuations  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...             9   \n",
       "1      ham                      Ok lar... Joking wif u oni...             6   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...             5   \n",
       "3      ham  U dun say so early hor... U c already then say...             6   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...             2   \n",
       "...    ...                                                ...           ...   \n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...             9   \n",
       "5568   ham              Will Ì_ b going to esplanade fr home?             1   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...             7   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...             1   \n",
       "5571   ham                         Rofl. Its true to its name             1   \n",
       "\n",
       "      Phonenumbers  Links  Uppercase  unusualwords  000   10  150p  ...  \\\n",
       "0                0      0          0             3  0.0  0.0   0.0  ...   \n",
       "1                0      0          0             3  0.0  0.0   0.0  ...   \n",
       "2                1      0          2             5  0.0  0.0   0.0  ...   \n",
       "3                0      0          2             1  0.0  0.0   0.0  ...   \n",
       "4                0      0          1             3  0.0  0.0   0.0  ...   \n",
       "...            ...    ...        ...           ...  ...  ...   ...  ...   \n",
       "5567             1      0          2             0  0.0  0.0   0.0  ...   \n",
       "5568             0      0          1             1  0.0  0.0   0.0  ...   \n",
       "5569             0      0          0             1  0.0  0.0   0.0  ...   \n",
       "5570             0      0          1             3  0.0  0.0   0.0  ...   \n",
       "5571             0      0          0             1  0.0  0.0   0.0  ...   \n",
       "\n",
       "         world  www  xmas  xxx   ya  yeah  year  yes   yo  yup  \n",
       "0     0.594379  0.0   0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "1     0.000000  0.0   0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "2     0.000000  0.0   0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "3     0.000000  0.0   0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "4     0.000000  0.0   0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "...        ...  ...   ...  ...  ...   ...   ...  ...  ...  ...  \n",
       "5567  0.000000  0.0   0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "5568  0.000000  0.0   0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "5569  0.000000  0.0   0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "5570  0.000000  0.0   0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "5571  0.000000  0.0   0.0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5572 rows x 307 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_extra_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c664ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1276cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X=data_extra_features\n",
    "features = X.columns.drop([\"Value\",\"Text\"])#\"Value_num\"\n",
    "target = [\"Value\"]\n",
    "X_train,X_test,y_train,y_test = train_test_split(X[features],X[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "673272f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9837281646326872\n",
      "0.9755922469490309\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "dt = DecisionTreeClassifier(min_samples_split=40)\n",
    "dt.fit(X_train,y_train)\n",
    "pred = dt.predict(X_test)\n",
    "print(accuracy_score(y_train, dt.predict(X_train)))\n",
    "print(accuracy_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db740c33",
   "metadata": {},
   "source": [
    "#  in the below code snippets, we are building 2 more classifier models - Naive Bayes Classifier and Maximum Entropy Classifier (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc927b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.964824120603015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9791816223977028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Building a Naive Bayes Model\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train,y_train)\n",
    "pred_mnb = mnb.predict(X_test)\n",
    "print(accuracy_score(y_test, pred_mnb))\n",
    "# Building a Logistic Regression Model\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "pred_lr = lr.predict(X_test)\n",
    "print(accuracy_score(y_test, pred_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa7d8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fcdfc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
